{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "folder = '/home/wilkinsonlab/Escritorio/elsevier/statements/'\n",
    "\n",
    "INPUT_FILE = folder+\"st_45_ground_truth_participants_3l.csv\"\n",
    "\n",
    "texts_test = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels_test = []  # list of label ids\n",
    "fin = codecs.open(INPUT_FILE,\"r\",  encoding='utf8')\n",
    "for line in fin:\n",
    "    sent, certain = line.strip().split(\"\\t\")\n",
    "    sent = [x for x in nltk.word_tokenize(sent) if x not in stopwords]\n",
    "    texts_test.append(' '.join(sent))\n",
    "    labels_test.append(certain)\n",
    "\n",
    "    \n",
    "print(len(texts_test))\n",
    "print(len(labels_test))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "texts_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#labels = to_categorical(np.asarray(labels))\n",
    "labels_test = np.asarray(labels_test, dtype=int)\n",
    "labels_test = list(labels_test)\n",
    "for index, item in enumerate(labels_test):\n",
    "    if item == 1:\n",
    "        labels_test[index] = [1,0,0]\n",
    "    elif item == 2:\n",
    "        labels_test[index] = [0,1,0]\n",
    "    else:\n",
    "        labels_test[index] = [0,0,1]\n",
    "labels_test = np.asarray(labels_test)\n",
    "# labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.05\n",
    "MAX_NB_WORDS = 6660\n",
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(\"/home/wilkinsonlab/Escritorio/elsevier/\", 'dictionary-200.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Indexing word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "import np_utils\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import numpy\n",
    "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.models import Model\n",
    "from keras.layers.pooling import GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.layers import LSTM, Bidirectional, Concatenate, GRU, CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers.merge import concatenate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import Embedding\n",
    "from keras import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import regularizers\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "# from pylab import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "puntuacion = 0.80\n",
    "folder = '/home/wilkinsonlab/Escritorio/elsevier/statements/'\n",
    "\n",
    "INPUT_FILE = folder+\"st_3221_ground_truth_train+val_3l.csv\"\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "fin = codecs.open(INPUT_FILE,\"r\",  encoding='utf8')\n",
    "maxlen = 0\n",
    "for line in fin:\n",
    "    sent, certain = line.strip().split(\"\\t\")\n",
    "    sent = [x for x in nltk.word_tokenize(sent) if x not in stopwords]\n",
    "    texts.append(' '.join(sent))\n",
    "    labels.append(certain)\n",
    "\n",
    "    if len(sent) > maxlen:\n",
    "        maxlen = len(sent)\n",
    "fin.close()   \n",
    "print(len(texts))\n",
    "print(len(labels))\n",
    "MAX_SEQUENCE_LENGTH = maxlen\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#labels = to_categorical(np.asarray(labels))\n",
    "labels = np.asarray(labels, dtype=int)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=20, shuffle=True)\n",
    "cvscores = []\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvscores = []\n",
    "np.random.seed(1)\n",
    "historial = []\n",
    "train_test= []\n",
    "contador = 0\n",
    "for train, test in kfold.split(data, labels):\n",
    "    if contador == 0:\n",
    "        labels = list(labels)\n",
    "        for index, item in enumerate(labels):\n",
    "            if item == 1:\n",
    "                labels[index] = [1,0,0]\n",
    "            elif item == 2:\n",
    "                labels[index] = [0,1,0]\n",
    "            else:\n",
    "                labels[index] = [0,0,1]\n",
    "        labels = np.asarray(labels)\n",
    "        contador = contador + 1\n",
    "    train_test.append([train,test])\n",
    "    #     reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, mode='auto', patience=3, min_lr=0.002)\n",
    "    stop = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3, verbose=1, mode='auto', restore_best_weights=True)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, EMBEDDING_DIM, \n",
    "                            input_length=MAX_SEQUENCE_LENGTH))\n",
    "    model.layers[0].set_weights([embedding_matrix])\n",
    "    model.layers[0].trainable = False\n",
    "    model.add(Dense(300,kernel_regularizer=regularizers.l1(0.000001)))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(200,kernel_regularizer=regularizers.l1(0.000001)))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(100,kernel_regularizer=regularizers.l1(0.000001)))\n",
    "    model.add((LSTM(50, dropout=0.1, recurrent_dropout=0.1)))#, kernel_regularizer=regularizers.l1(0.00001))))\n",
    "    model.add(Dense(3,kernel_regularizer=regularizers.l1(0.000001)))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", \n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit(data[train], labels[train], batch_size=20, \n",
    "                            epochs=10\n",
    "                           ,validation_data=(data[test], labels[test]), callbacks=[stop])\n",
    "    historial.append(history)\n",
    "    # evaluate model\n",
    "    scores = model.evaluate(data[test], labels[test], verbose=1)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(scores[0], scores[1]))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "print cvscores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
